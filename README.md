# ML-Model-Evaluations
This project evaluates different machine learning models on the dataset and looks into hyper-parameter optimization of those models.
1. Data Preprocessing
Data preprocessing shapes the raw dataset into a format that is more suitable for machine learning models. This process improves model accuracy, efficiency, and effectiveness. Below is an in-depth explanation of the preprocessing steps undertaken for the "Pump it Up: Data Mining the Water Table" competition dataset.
Categorical Feature Handling
The dataset comprises several categorical features with varying levels of cardinality. Handling these appropriately is essential for model performance:
OneHotEncoder: This method was applied to categorical variables with relatively low cardinality. It transforms each category value into a new binary column, ensuring the model comprehensively understands the presence or absence of categories in the data.
OrdinalEncoder: For ordinal data, where the order of categories matters, this encoder was used to transform string labels to integer codes, preserving the inherent order of the categories which might carry significance for model prediction.
TargetEncoder: High cardinality features pose a challenge due to the vast number of categories. Target encoding helps by replacing a categorical value with a blend of the posterior probability of the target given a particular categorical value and the prior probability of the target over all the data. However, to prevent data leakage and overfitting that might occur due to using target information in the training data, a more complex pipeline was needed. This involved a CustomPreprocessor for explicit data preparation, filling missing values, and finally applying the TargetEncoder. This approach helped in encoding high cardinality categorical features more meaningfully without leaking target information.
Dealing with Missing Values
200024484
Missing data can significantly impact the performance of machine learning models. Two primary strategies were employed:
For numerical data, a SimpleImputer with a strategy of replacing missing values with the median was used, assuming numerical features are generally continuous and the median provides a robust measure of central tendency.
For categorical data, missing values were replaced with the most frequent category, again using SimpleImputer. This method assumes that the most common category can serve as a reasonable guess for missing data.
Scaling Numerical Values
Numerical features vary in range and distribution. To ensure that all features contribute equally to the model's prediction, StandardScaler was applied. This scaler removes the mean and scales features to unit variance. This step is crucial, especially for models like Logistic Regression, where scale and distribution of variables can significantly influence the model's convergence and performance.
Datetime Features Handling
Datetime features can encode valuable temporal information. In this dataset, the DateTransformer custom transformer was used to extract the year from datetime features. By focusing on the year component, we aimed to capture any long-term trends or effects related to the age or time-related attributes of the water points, such as construction year, which could influence their functionality.
Other Transformers
DataFrameSelector: A custom transformer to select subsets of data for different processing pipelines. It aids in segregating features based on their data type or relevance before applying specific transformations.
CustomPreprocessor: Specifically designed for preparing categorical data before target encoding. This transformer ensures all categorical columns are of type object and fills missing values with a placeholder, thus standardizing the preprocessing steps across different categorical features.
Label Encoding
The target variable, indicating the functionality of water points, is categorical. The LabelEncoder was utilized to transform this target into numerical form suitable for model training. This encoding is straightforward but essential for model compatibility, as most machine learning algorithms require numerical inputs.
2. Design
Implementing machine learning models for the competition data involved making design decisions regarding preprocessing methods and model hyper-parameters. These choices significantly impact the model's ability to learn from the data and perform accurately on unseen data.
Preprocessing Method Parameters
200024484
Scaling Numerical Values: StandardScaler was chosen to scale numerical features. This decision was based on the need to normalize the features so that each has a mean of zero and a standard deviation of one. This scaling is particularly important for algorithms like Logistic Regression and MLPClassifier, where gradient descent is used, and feature scaling can greatly affect convergence speed.
Encoding Categorical Features: The choice between OneHotEncoder, OrdinalEncoder, and TargetEncoder depended on the feature's characteristics. For features with a low number of categories, OneHotEncoder was preferred to avoid introducing arbitrary ordinality. For ordinal features or those with a natural ranking, OrdinalEncoder was used. High cardinality features were handled with TargetEncoder to reduce the dimensionality and potential for overfitting while preserving information about the target variable.
Handling Missing Values: Missing values in both numerical and categorical features were imputed using SimpleImputer. For numerical features, the median value was used because it is robust to outliers. For categorical features, the mode (most frequent category) was used as a simple and effective way to impute missing values.
Model Hyper-parameters
RandomForestClassifier: Hyper-parameters like n_estimators, max_depth, min_samples_split, and min_samples_leaf were optimized. The number of trees (n_estimators) directly influences the model's ability to generalize but also increases computational cost. A range of 50 to 400 was explored. max_depth controls the depth of each tree, helping prevent overfitting when set appropriately. The min_samples_split and min_samples_leaf parameters help control the growth of the trees, ensuring they don't grow too complex and overfit the training data.
LogisticRegression: The key hyper-parameter optimized was C, the inverse regularization strength. Regularization is crucial in logistic regression to prevent overfitting, especially in high-dimensional datasets. The C parameter was varied over a wide range to find the optimal balance between complexity and generalization capability. A logarithmic scale from 1e-10 to 1e10 was used for exploration.
GradientBoostingClassifier and HistGradientBoostingClassifier: For these models, the number of boosting stages (n_estimators) and the learning rate were considered critical hyper-parameters. A higher number of stages can lead to better performance but risks overfitting, while the learning rate controls the contribution of each tree, with smaller rates requiring more trees but potentially leading to a more robust model.
